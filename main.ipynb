{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e807e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime, date, time, timedelta \n",
    "from pandas import concat, Series, DataFrame, MultiIndex, to_datetime, Timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81f00a57",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fetch_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocess_data\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m preprocess_data, create_sequences \n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LSTMModel, train_model \n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevaluate_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m evaluate_model \u001b[38;5;28;01mas\u001b[39;00m evaluate_model_standalone \n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/Quantum_LSTM_Strategy/core/preprocess_data.py:11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ceil\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Import fetch_data from a separate file\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfetch_data\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fetch_data\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# --- Data Preprocessing ---\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34madd_indicators\u001b[39m(ticker, add_all_features=\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'fetch_data'"
     ]
    }
   ],
   "source": [
    "from core.preprocess_data import preprocess_data, create_sequences \n",
    "from core.modeling import LSTMModel, train_model \n",
    "from core.evaluate_model import evaluate_model as evaluate_model_standalone \n",
    "from core.simulation import run_simulation_strategy \n",
    "from core.report import generate_simulation_report \n",
    "from core.optimizer import calculate_expected_returns_covariance, mean_variance_optimization_cvxpy, mean_variance_optimization_scipy, generate_efficient_frontier \n",
    "from core.risk_control import check_stop_loss, enforce_cash_limit, rebalance_portfolio \n",
    "from core.metrics import calculate_max_drawdown, calculate_sharpe_ratio, calculate_cvar, portfolio_performance \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce067a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Run stock trading strategy workflow.\")\n",
    "    parser.add_argument('--tickers', nargs='+', default=['AAPL', 'MSFT'], help='List of tickers to process.')\n",
    "    parser.add_argument('--sequence_length', type=int, default=20, help='Number of time steps in each sequence.')\n",
    "    parser.add_argument('--initial_cash', type=float, default=10000, help='Initial cash for simulation.')\n",
    "    parser.add_argument('--trading_ticker', type=str, default='AAPL', help='Ticker for trading simulation.')\n",
    "    parser.add_argument('--hidden_dim', type=int, default=64, help='Number of units in LSTM hidden layers.')\n",
    "    parser.add_argument('--dropout_prob', type=float, default=0.3, help='Dropout rate for regularization.')\n",
    "    parser.add_argument('--lr', type=float, default=0.001, help='Learning rate for the optimizer.')\n",
    "    parser.add_argument('--epochs', type=int, default=50, help='Number of training epochs.')\n",
    "    parser.add_argument('--batch_size', type=int, default=32, help='Batch size for training.')\n",
    "    parser.add_argument('--model_path', type=str, default='models/best_model.pth', help='Path to save/load the best model state dictionary.')\n",
    "    parser.add_argument('--config_path', type=str, default='models/model_config.json', help='Path to save/load the model configuration.')\n",
    "    parser.add_argument('--skip_training', action='store_true', help='Skip model training and only run simulation with existing model.')\n",
    "    parser.add_argument('--output_dir', type=str, default='results', help='Directory to save output files (e.g., equity curve plot).')\n",
    "    parser.add_argument('--core_dir', type=str, default='core', help='Directory to save core metrics.')\n",
    "    parser.add_argument('--log_evaluation_details', action='store_true', help='Log detailed evaluation results.')\n",
    "    # Add arguments for other optional components\n",
    "    parser.add_argument('--run_portfolio_optimization', action='store_true', help='Run classical portfolio optimization after simulation.')\n",
    "    parser.add_argument('--enable_risk_control_analysis', action='store_true', help='Perform risk control analysis after simulation (e.g., calculate drawdown/CVaR).')\n",
    "    parser.add_argument('--quantum_enabled', action='store_true', help='Enable quantum-inspired/quantum features if available.')\n",
    "\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Create output and core directories if they don't exist\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    os.makedirs(args.core_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    # --- 1. Preprocess the data ---\n",
    "    # preprocess_data is imported from preprocess_data.py\n",
    "    X_scaled, y, scaler, df_processed, sequence_dates, processed_tickers = preprocess_data(args.tickers, args.sequence_length)\n",
    "\n",
    "    if X_scaled is not None and y is not None and not df_processed.empty and processed_tickers:\n",
    "        print(\"\\nData Preprocessing Successful.\")\n",
    "\n",
    "        input_dim = X_scaled.shape[-1] # Determine input_dim from preprocessed data\n",
    "        print(f\"Automatically set input_dim based on data shape: {input_dim}\")\n",
    "\n",
    "        X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "        # Determine the date range for the \"current dataset\" (evaluation data)\n",
    "        eval_size = int(0.2 * len(X_tensor)) # Assuming 20% for evaluation\n",
    "        train_size = len(X_tensor) - eval_size\n",
    "\n",
    "        X_train_eval = X_tensor[:train_size] # Data used for main training/evaluation split\n",
    "        y_train_eval = y_tensor[:train_size]\n",
    "        sequence_dates_train_eval = sequence_dates[:train_size] # Dates corresponding to the end of each sequence\n",
    "\n",
    "        X_eval = X_tensor[train_size:] # Data used for final evaluation/simulation\n",
    "        y_eval = y_tensor[train_size:]\n",
    "        sequence_dates_eval = sequence_dates[train_size:]\n",
    "\n",
    "\n",
    "        # --- Prepare Fine-tuning Data ---\n",
    "        X_finetune, y_finetune = None, None\n",
    "        if len(sequence_dates_train_eval) > args.sequence_length:\n",
    "             # Find the end date of the main training/evaluation data\n",
    "             end_date_train_eval = sequence_dates_train_eval[-1]\n",
    "\n",
    "             # Define the fine-tuning date range: 2 months to a year before the current dataset (end of X_eval)\n",
    "             # The \"current dataset\" is interpreted as the final evaluation set here.\n",
    "             # So, the range is 2 months to 1 year before the end of X_eval.\n",
    "             if sequence_dates_eval: # Ensure eval dates exist\n",
    "                 end_date_eval = sequence_dates_eval[-1]\n",
    "                 fine_tune_end_date = end_date_eval - pd.DateOffset(months=2)\n",
    "                 fine_tune_start_date = end_date_eval - pd.DateOffset(years=1)\n",
    "\n",
    "                 print(f\"\\nPreparing fine-tuning data between {fine_tune_start_date.strftime('%Y-%m-%d')} and {fine_tune_end_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "                 # Filter the original processed dataframe to this date range\n",
    "                 # Need to be careful with date indexing and timezone if present\n",
    "                 try:\n",
    "                     df_finetune_range = df_processed.loc[fine_tune_start_date:fine_tune_end_date]\n",
    "                     print(f\"Filtered dataframe for fine-tuning shape: {df_finetune_range.shape}\")\n",
    "\n",
    "                     if not df_finetune_range.empty:\n",
    "                         # Re-create sequences specifically for the fine-tuning date range\n",
    "                         # Need the common features used during the initial preprocessing\n",
    "                         # Assumes create_sequences is also imported from preprocess_data.py\n",
    "                         inner_cols = [df_processed[t].columns.tolist() for t in df_processed.columns.levels[0]]\n",
    "                         common_features = list(reduce(set.intersection, map(set, inner_cols)))\n",
    "                         if 'Target' in common_features:\n",
    "                             common_features.remove('Target')\n",
    "\n",
    "                         X_finetune_np, y_finetune_np, sequence_dates_finetune = create_sequences(\n",
    "                             df_finetune_range, common_features, args.sequence_length\n",
    "                         )\n",
    "\n",
    "                         if X_finetune_np.size > 0 and y_finetune_np.size > 0:\n",
    "                              # Scale the fine-tuning data using the same scaler fitted on the main dataset\n",
    "                              # Reshape X_finetune_np for scaling\n",
    "                              X_finetune_reshaped_for_scaling = X_finetune_np.reshape(-1, X_finetune_np.shape[-1])\n",
    "\n",
    "                            # Ensure the number of features matches the scaler's expected input\n",
    "                             if hasattr(scaler, 'n_features_in_') and X_finetune_reshaped_for_scaling.shape[1] == scaler.n_features_in_:\n",
    "                                   scaled_X_finetune_reshaped = scaler.transform(X_finetune_reshaped_for_scaling)\n",
    "                                   # Reshape back to (samples, timesteps, features)\n",
    "                                   X_finetune = torch.tensor(scaled_X_finetune_reshaped.reshape(X_finetune_np.shape[0], args.sequence_length, X_finetune_np.shape[-1]), dtype=torch.float32)\n",
    "                                   y_finetune = torch.tensor(y_finetune_np, dtype=torch.float32)\n",
    "                                   print(f\"Fine-tuning data prepared. Shape of X_finetune: {X_finetune.shape}, Shape of y_finetune: {y_finetune.shape}\")\n",
    "                             elif not hasattr(scaler, 'n_features_in_'):\n",
    "                                   print(\"Warning: scaler.n_features_in_ not available. Cannot verify feature count for fine-tuning scaling. Attempting scaling.\")\n",
    "                                   scaled_X_finetune_reshaped = scaler.transform(X_finetune_reshaped_for_scaling)\n",
    "                                   X_finetune = torch.tensor(scaled_X_finetune_reshaped.reshape(X_finetune_np.shape[0], args.sequence_length, X_finetune_np.shape[-1]), dtype=torch.float32)\n",
    "                                   y_finetune = torch.tensor(y_finetune_np, dtype=torch.float32)\n",
    "                                   print(f\"Fine-tuning data prepared (scaling not verified). Shape of X_finetune: {X_finetune.shape}, Shape of y_finetune: {y_finetune.shape}\")\n",
    "                             else:\n",
    "                                   print(f\"Warning: Feature count mismatch for fine-tuning data scaling. Expected {scaler.n_features_in_}, got {X_finetune_reshaped_for_scaling.shape[1]}. Skipping fine-tuning data preparation.\")\n",
    "                                   X_finetune, y_finetune = None, None # Reset to None if scaling fails\n",
    "\n",
    "                         else:\n",
    "                              print(\"Warning: No sequences created for the fine-tuning date range.\")\n",
    "\n",
    "                     else:\n",
    "                         print(\"Warning: Filtered dataframe for fine-tuning date range is empty.\")\n",
    "\n",
    "                 except KeyError:\n",
    "                     print(f\"Warning: Could not filter dataframe for fine-tuning range using dates {fine_tune_start_date} to {fine_tune_end_date}. Check date index.\")\n",
    "                 except Exception as e:\n",
    "                     print(f\"An error occurred during fine-tuning data preparation: {e}\")\n",
    "             else:\n",
    "                 print(\"Warning: Evaluation dates are not available. Cannot determine date range for fine-tuning.\")\n",
    "\n",
    "\n",
    "        print(f\"\\nTraining set size (main): {len(X_train_eval)}\") # Note: This is the data *before* final eval split\n",
    "        print(f\"Evaluation set size (final): {len(X_eval)}\")\n",
    "\n",
    "\n",
    "        # --- 2. Train or Load the Model ---\n",
    "        trained_model = None # Initialize model variable\n",
    "\n",
    "        if not args.skip_training:\n",
    "             print(\"\\n--- Model Training ---\")\n",
    "             # train_model is imported from modeling.py\n",
    "             trained_model, training_losses = train_model(\n",
    "                X_train_eval, y_train_eval, # Main training/evaluation split data\n",
    "                X_eval, y_eval, # Final evaluation data\n",
    "                X_finetune, y_finetune, # Fine-tuning data (can be None)\n",
    "                input_dim, args.hidden_dim, args.output_dim, args.dropout_prob,\n",
    "                args.lr, args.epochs, args.batch_size, args.model_path, args.config_path\n",
    "             )\n",
    "             print(\"\\nModel Training Complete.\")\n",
    "             # Optional: Plot training loss from script (requires matplotlib and a plotting function)\n",
    "             if 'plot_training_loss' in globals(): # Check if plotting function is available\n",
    "                plot_training_loss(training_losses, os.path.join(args.output_dir, 'training_loss.png'))\n",
    "\n",
    "\n",
    "        elif os.path.exists(args.model_path) and os.path.exists(args.config_path):\n",
    "            print(f\"\\n--- Loading Pre-trained Model ---\")\n",
    "            try:\n",
    "                # Load config to get model dimensions\n",
    "                with open(args.config_path, 'r') as f:\n",
    "                    config = json.load(f)\n",
    "                # Use data-derived input_dim if not in config, fallback to argument default\n",
    "                loaded_input_dim = config.get('input_dim', input_dim if input_dim is not None else args.hidden_dim) # Fallback to hidden_dim is likely wrong, should be input_dim\n",
    "                loaded_hidden_dim = config.get('hidden_dim', args.hidden_dim)\n",
    "                loaded_output_dim = config.get('output_dim', args.output_dim)\n",
    "                loaded_dropout_prob = config.get('dropout_prob', args.dropout_prob)\n",
    "\n",
    "                # LSTMModel is imported from modeling.py\n",
    "                trained_model = LSTMModel(loaded_input_dim, loaded_hidden_dim, loaded_output_dim, loaded_dropout_prob)\n",
    "                device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                trained_model.load_state_dict(torch.load(args.model_path, map_location=device))\n",
    "                trained_model.to(device)\n",
    "                print(f\"Successfully loaded model from {args.model_path} and config from {args.config_path}\")\n",
    "                # Evaluate the loaded model using the standalone evaluation script\n",
    "                # evaluate_model_standalone is imported from evaluate_model.py\n",
    "                eval_accuracy = evaluate_model_standalone(trained_model, X_eval, y_eval, args.core_dir, args.output_dir, args.log_evaluation_details)\n",
    "                print(f\"Loaded Model Evaluation Accuracy (on final eval set): {eval_accuracy:.4f}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading model or config: {e}. Cannot run simulation.\")\n",
    "                trained_model = None # Ensure trained_model is None if loading fails\n",
    "        else:\n",
    "            print(f\"\\n--- Skipping Training ---\")\n",
    "            print(f\"Model file '{args.model_path}' or config file '{args.config_path}' not found. Cannot run simulation without training or a loaded model.\")\n",
    "\n",
    "\n",
    "        # --- 3. Run Simulation ---\n",
    "        if trained_model is not None:\n",
    "             print(\"\\n--- Trading Simulation ---\")\n",
    "             # Select a ticker for simulation from the successfully processed tickers\n",
    "             trading_ticker_for_simulation = args.trading_ticker if args.trading_ticker in processed_tickers else (processed_tickers[0] if processed_tickers else None)\n",
    "\n",
    "             if trading_ticker_for_simulation:\n",
    "                 if trading_ticker_for_simulation != args.trading_ticker:\n",
    "                      print(f\"Warning: Specified trading ticker '{args.trading_ticker}' not in processed tickers. Using '{trading_ticker_for_simulation}' for simulation.\")\n",
    "\n",
    "                 # Need to get the common_features used during preprocessing to pass to run_simulation_strategy\n",
    "                 # Assuming df_processed structure is maintained from preprocess_data call\n",
    "                 inner_cols = [df_processed[t].columns.tolist() for t in df_processed.columns.levels[0]]\n",
    "                 simulation_common_features = list(reduce(set.intersection, map(set, inner_cols)))\n",
    "                 if 'Target' in simulation_common_features:\n",
    "                     simulation_common_features.remove('Target')\n",
    "\n",
    "                 # run_simulation_strategy is imported from simulation.py\n",
    "                 final_equity, trades, equity_curve, simulation_dates_sim = run_simulation_strategy( # Renamed to avoid conflict\n",
    "                     df_processed, trained_model, scaler, simulation_common_features,\n",
    "                     args.sequence_length, args.initial_cash, trading_ticker_for_simulation, sequence_dates # Pass original sequence_dates for simulation start index logic\n",
    "                 )\n",
    "\n",
    "                 print(f\"\\nSimulation Final Equity: {final_equity:.2f}\")\n",
    "                 print(f\"Number of Trades: {len(trades)}\")\n",
    "\n",
    "                 # Save trades and equity curve data (handled within run_simulation_strategy now)\n",
    "\n",
    "\n",
    "                 # --- 4. Generate Report ---\n",
    "                 print(\"\\n--- Generating Simulation Report ---\")\n",
    "                 # generate_simulation_report is imported from report.py\n",
    "                 generate_simulation_report(\n",
    "                     simulation_trades_path=os.path.join(args.output_dir, 'simulation_trades.csv'),\n",
    "                     equity_curve_path=os.path.join(args.output_dir, 'simulation_equity_curve.csv'),\n",
    "                     output_dir=args.output_dir\n",
    "                 )\n",
    "\n",
    "\n",
    "                 # --- 5. Portfolio Optimization (Optional) ---\n",
    "                 if args.run_portfolio_optimization:\n",
    "                      print(\"\\n--- Running Classical Portfolio Optimization ---\")\n",
    "                      # Assuming you want to optimize based on the returns of the tickers used in preprocessing\n",
    "                      try:\n",
    "                          returns_df = df_processed.xs('Returns', level=1, axis=1)\n",
    "                          if not returns_df.empty:\n",
    "                              # calculate_expected_returns_covariance is imported from config.optimizer\n",
    "                              expected_returns, covariance_matrix = calculate_expected_returns_covariance(returns_df)\n",
    "                              if not expected_returns.empty and not covariance_matrix.empty:\n",
    "                                  # Example: Max Sharpe Ratio\n",
    "                                  # mean_variance_optimization_cvxpy is imported from config.optimizer\n",
    "                                  # Save results to core/optimizer_results\n",
    "                                  optimal_weights_sharpe, perf_sharpe, _ = mean_variance_optimization_cvxpy(expected_returns, covariance_matrix, objective='max_sharpe', save_path_prefix=os.path.join(args.core_dir, 'optimizer_results/cvxpy_'))\n",
    "                                  print(f\"\\nMax Sharpe Portfolio (CVXPY/SciPy): Return={perf_sharpe[0]:.2f}%, Volatility={perf_sharpe[1]:.2f}%, Sharpe={perf_sharpe[2]:.4f}\")\n",
    "                                  # Optional: Print optimal weights\n",
    "                                  print(\"Optimal Weights:\", dict(zip(expected_returns.index, optimal_weights_sharpe)))\n",
    "\n",
    "                                  # Minimize Volatility using SciPy\n",
    "                                  # mean_variance_optimization_scipy is imported from config.optimizer\n",
    "                                  # Save results to core/optimizer_results\n",
    "                                  optimal_weights_min_vol, perf_min_vol, _ = mean_variance_optimization_scipy(expected_returns, covariance_matrix, objective='min_volatility', save_path_prefix=os.path.join(args.core_dir, 'optimizer_results/scipy_'))\n",
    "                                  print(f\"\\nMin Volatility Portfolio (SciPy): Return={perf_min_vol[0]:.2f}%, Volatility={perf_min_vol[1]:.2f}%, Sharpe={perf_min_vol[2]:.4f}\")\n",
    "\n",
    "                                  # Generate and plot Efficient Frontier\n",
    "                                  # generate_efficient_frontier is imported from config.optimizer\n",
    "                                  # Save frontier to core/optimizer_results\n",
    "                                  frontier_df = generate_efficient_frontier(expected_returns, covariance_matrix, save_path=os.path.join(args.core_dir, 'optimizer_results/efficient_frontier.json'))\n",
    "                                  if not frontier_df.empty:\n",
    "                                       # Calculate performance of equal weight portfolio for plotting reference\n",
    "                                       equal_weights = np.ones(len(expected_returns)) / len(expected_returns)\n",
    "                                       # portfolio_performance is imported from metrics.py\n",
    "                                       perf_equal = portfolio_performance(equal_weights, expected_returns, covariance_matrix)\n",
    "\n",
    "                                       # You could add plotting for the frontier here if you have a plotting function\n",
    "                                       # plot_efficient_frontier(frontier_df, perf_sharpe, perf_min_vol, perf_equal)\n",
    "\n",
    "                                  else:\n",
    "                                       print(\"Could not generate efficient frontier.\")\n",
    "\n",
    "                              else:\n",
    "                                  print(\"Could not calculate expected returns or covariance matrix for optimization.\")\n",
    "                          else:\n",
    "                              print(\"Returns data is empty. Cannot perform portfolio optimization.\")\n",
    "                      except Exception as e:\n",
    "                          print(f\"An error occurred during portfolio optimization: {e}\")\n",
    "\n",
    "\n",
    "                 # --- 6. Risk Control Analysis (Optional) ---\n",
    "                 # This is for analysis *after* simulation, not during the simulation loop itself.\n",
    "                 if args.enable_risk_control_analysis:\n",
    "                      print(\"\\n--- Running Risk Control Analysis ---\")\n",
    "                      # Example: Calculate Max Drawdown and CVaR using metrics.py\n",
    "                      if equity_curve: # Check if simulation generated an equity curve\n",
    "                           # calculate_max_drawdown and calculate_cvar are imported from metrics.py\n",
    "                           max_dd = calculate_max_drawdown(equity_curve)\n",
    "                           # Need returns for CVaR\n",
    "                           equity_returns = pd.Series(equity_curve).pct_change().dropna()\n",
    "                           cvar_val = calculate_cvar(equity_returns)\n",
    "\n",
    "                           print(f\"\\nSimulation Max Drawdown: {max_dd:.2%}\")\n",
    "                           print(f\"Simulation CVaR (95%): {cvar_val:.4f}\")\n",
    "\n",
    "                           # You could also add checks for stop-loss trigger points or cash limits\n",
    "                           # based on the historical simulation data here, but this is less\n",
    "                           # about *applying* risk control and more about *analyzing* the simulated\n",
    "                           # strategy's risk profile.\n",
    "                    else: print(\"No equity curve available for risk control analysis.\")\n",
    "            else: print(\"Error: No valid ticker available for simulation.\")\n",
    "        else: print(\"\\nSkipping simulation as model training failed or model could not be loaded.\")\n",
    "    else: print(\"\\nData preprocessing failed or no tickers were processed. Cannot proceed with model training and simulation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
